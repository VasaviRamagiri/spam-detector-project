!pip install tensorflow transformers sklearn pyswarm pandas numpy --quiet
!pip install pyswarm


import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow.keras import layers, models
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from pyswarm import pso
from transformers import BertTokenizer
import matplotlib.pyplot as plt


df = pd.read_csv("messages.csv")

df['text'] = (df['subject'].astype(str) + " " + df['message'].astype(str)).fillna('')
labels = df['label'].values

df['subject_length'] = df['subject'].apply(lambda x: len(str(x)))
df['message_length'] = df['message'].apply(lambda x: len(str(x)))
df['num_special_chars'] = df['text'].apply(lambda x: sum(c in "!@#$%^&*()_+" for c in str(x)))
df['num_digits'] = df['text'].apply(lambda x: sum(c.isdigit() for c in str(x)))
df['num_uppercase'] = df['text'].apply(lambda x: sum(c.isupper() for c in str(x)))
df['avg_word_length'] = df['text'].apply(lambda x: np.mean([len(w) for w in x.split()]) if x.split() else 0)
df['num_words'] = df['text'].apply(lambda x: len(x.split()))
df['has_link'] = df['text'].apply(lambda x: int("http" in x or "www" in x))
df['has_reply_or_forward'] = df['subject'].apply(lambda x: int("Re:" in str(x) or "Fwd:" in str(x)))
features = ['subject_length', 'message_length', 'num_special_chars', 'num_digits',
            'num_uppercase', 'avg_word_length', 'num_words', 'has_link', 'has_reply_or_forward']


scaler = MinMaxScaler()
df[features] = scaler.fit_transform(df[features])


tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
max_len = 128

def bert_tokenize(texts):
    return tokenizer(
        texts.tolist(), padding='max_length', truncation=True, max_length=max_len, return_tensors='tf'
    )['input_ids'].numpy()

tokenized_texts = bert_tokenize(df['text'])


X = np.hstack((tokenized_texts, df[features].values))
y = labels


train_x, test_x, train_y, test_y = train_test_split(X, y, test_size=0.2, random_state=42)


def build_gru_model(units, learning_rate):
    model = models.Sequential([
        layers.Embedding(input_dim=30522, output_dim=128, input_length=max_len + len(features)),
        layers.GRU(int(units), return_sequences=True),
        layers.GRU(int(units)),
        layers.Dense(64, activation='relu'),
        layers.Dropout(0.3),
        layers.Dense(1, activation='sigmoid')
    ])
    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)
    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])
    return model


def objective_function(params):
    units, learning_rate = params
    model = build_gru_model(units, learning_rate)
    history = model.fit(train_x, train_y, epochs=1, batch_size=16, validation_data=(test_x, test_y), verbose=0)
    val_acc = max(history.history['val_accuracy'])
    return -val_acc

lb = [32, 0.0005]
ub = [128, 0.005]
best_params, _ = pso(objective_function, lb, ub, swarmsize=8, maxiter=5)
best_units, best_lr = best_params


final_model = build_gru_model(best_units, best_lr)
final_model.fit(train_x, train_y, epochs=5, batch_size=32, validation_data=(test_x, test_y))

test_loss, test_acc = final_model.evaluate(test_x, test_y)

import time  # Import the time module

# For GRU + PSO
start_time_gru = time.time()
# Assuming 'final_model' is the trained model from previous cell
final_model.fit(train_x, train_y, epochs=5, batch_size=32, validation_data=(test_x, test_y))  # your GRU training code
end_time_gru = time.time()
gru_training_time = end_time_gru - start_time_gru
print(f"GRU Training Time: {gru_training_time} seconds") # Print training time
