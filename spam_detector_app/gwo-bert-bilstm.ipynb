import pandas as pd
from transformers import BertTokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from sklearn.model_selection import train_test_split
import tensorflow as tf
from tensorflow.keras import layers, models
import numpy as np

with open('messages.csv', 'r') as file:
    lines = file.readlines()
    print(f"Total lines in the file: {len(lines)}")

try:
    chunk_size = 500
    chunks = pd.read_csv('messages.csv', chunksize=chunk_size)
    for chunk in chunks:
        print(chunk.head())
except pd.errors.ParserError as e:
    print(f"ParserError: {e}")

try:
    df = pd.read_csv('messages.csv', engine='python', on_bad_lines='skip')
    print("File loaded successfully!")
    print(df.head())
except pd.errors.ParserError as e:
    print(f"Error reading file: {e}")


tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
df['text'] = (df['subject'].astype(str) + " " + df['message'].astype(str)).fillna('')
inputs = tokenizer(df['text'].tolist(), padding=True, truncation=True, max_length=512, return_tensors='tf')
labels = df['label'].values

input_ids = inputs['input_ids'].numpy()
train_inputs, val_inputs, train_labels, val_labels = train_test_split(
    input_ids, labels, test_size=0.2, random_state=42
)
train_inputs = pad_sequences(train_inputs, padding='post', maxlen=512)
val_inputs = pad_sequences(val_inputs, padding='post', maxlen=512)



def fitness_function(feature_mask, X, y):
    selected_indices = np.where(feature_mask == 1)[0]
    if len(selected_indices) == 0:
        return 0
    X_selected = X[:, selected_indices]
    X_train_fs, X_test_fs, y_train_fs, y_test_fs = train_test_split(X_selected, y, test_size=0.2, random_state=42)
    clf = tf.keras.Sequential([
        layers.Dense(10, activation='relu', input_shape=(len(selected_indices),)),
        layers.Dense(1, activation='sigmoid')
    ])
    clf.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    clf.fit(X_train_fs, y_train_fs, epochs=2, verbose=0)
    loss, accuracy = clf.evaluate(X_test_fs, y_test_fs, verbose=0)
    return accuracy

def gwo_feature_selection(X, y, num_wolves=5, max_iter=5):
    num_features = X.shape[1]
    wolves = np.random.randint(0, 2, (num_wolves, num_features))
    alpha_score = beta_score = delta_score = -1
    alpha_pos = beta_pos = delta_pos = np.zeros(num_features)
    for iter in range(max_iter):
        for i in range(num_wolves):
            fitness = fitness_function(wolves[i], X, y)
            if fitness > alpha_score:
                delta_score, delta_pos = beta_score, beta_pos.copy()
                beta_score, beta_pos = alpha_score, alpha_pos.copy()
                alpha_score, alpha_pos = fitness, wolves[i].copy()
            elif fitness > beta_score:
                delta_score, delta_pos = beta_score, beta_pos.copy()
                beta_score, beta_pos = fitness, wolves[i].copy()
            elif fitness > delta_score:
                delta_score, delta_pos = fitness, wolves[i].copy()
        a = 2 - iter * (2 / max_iter)
        for i in range(num_wolves):
            for j in range(num_features):
                r1, r2 = np.random.rand(), np.random.rand()
                A1 = 2 * a * r1 - a
                C1 = 2 * r2
                D_alpha = abs(C1 * alpha_pos[j] - wolves[i][j])
                X1 = alpha_pos[j] - A1 * D_alpha
                r1, r2 = np.random.rand(), np.random.rand()
                A2 = 2 * a * r1 - a
                C2 = 2 * r2
                D_beta = abs(C2 * beta_pos[j] - wolves[i][j])
                X2 = beta_pos[j] - A2 * D_beta
                r1, r2 = np.random.rand(), np.random.rand()
                A3 = 2 * a * r1 - a
                C3 = 2 * r2
                D_delta = abs(C3 * delta_pos[j] - wolves[i][j])
                X3 = delta_pos[j] - A3 * D_delta
                new_pos = (X1 + X2 + X3) / 3
                wolves[i][j] = 1 if new_pos > 0.5 else 0
    selected_indices = np.where(alpha_pos == 1)[0]
    X_selected = X[:, selected_indices]
    return X_selected, selected_indices


train_inputs_selected, selected_indices = gwo_feature_selection(train_inputs, train_labels, num_wolves=5, max_iter=3)
val_inputs_selected = val_inputs[:, selected_indices]


def build_model_with_selected_features(num_features):
    input_ids_layer = layers.Input(shape=(num_features,), dtype=tf.int32, name='input_ids')
    bert_model = tf.keras.Sequential([
        layers.InputLayer(input_shape=(num_features,), dtype=tf.int32),
        layers.Embedding(input_dim=tokenizer.vocab_size, output_dim=768, input_length=num_features),
        layers.Bidirectional(layers.LSTM(128, return_sequences=True))
    ])
    x = layers.Bidirectional(layers.LSTM(128))(bert_model(input_ids_layer))
    output = layers.Dense(1, activation='sigmoid')(x)
    model = models.Model(inputs=input_ids_layer, outputs=output)
    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    return model

model = build_model_with_selected_features(len(selected_indices))

history = model.fit(
    train_inputs_selected,
    train_labels,
    validation_data=(val_inputs_selected, val_labels),
    epochs=3,
    batch_size=32
)

loss, accuracy = model.evaluate(val_inputs_selected, val_labels)
print(f"Validation Accuracy after GWO: {accuracy * 100:.2f}%")
